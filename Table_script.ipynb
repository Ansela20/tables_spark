{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc89021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "       .appName(\"test\")\\\n",
    "           .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set table and database name\n",
    "tableName = \"xyz\"\n",
    "databaseName = \"abc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23995045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if table exists\n",
    "exists = spark.catalog.tableExists(f\"{databaseName}.{tableName}\")\n",
    "\n",
    "print(exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Get end date (previous month)\n",
    "end_date_df = spark.sql(\n",
    "  \"\"\" \n",
    "  SELECT DATE_TRUNC('MONTH', CURRENT_DATE - INTERVAL '1' MONTH) as end_date\n",
    "  \"\"\")\n",
    "end_date = end_date_df.first()[\"end_date\"]\n",
    "print(f\"END DATE: {end_date}\")\n",
    "\n",
    "\n",
    "if exists:\n",
    "    #Get last date of data loaded in table\n",
    "    start_date_df = spark.sql(\n",
    "    f\"\"\" \n",
    "    SELECT MAX(<date_field>) + INTERVAL '1' MONTH as start_date FROM {databaseName}.{tableName}\n",
    "    \"\"\")\n",
    "    \n",
    "    start_date = start_date_df.first()[\"start_date\"]\n",
    "    print(f\"START DATE: {start_date}\")\n",
    "else:\n",
    "    #Default to the beginning of reuired fiscal year\n",
    "    start_date = datetime.datetime(2022, 11, 1, 00, 00, 00)\n",
    "    print(f\"START DATE: {start_date}\")\n",
    "    \n",
    "if start_date > end_date:\n",
    "    dbutils.notebook.exit(\"Stopping the notebook execution because there is no need to update the table\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the query results from source onto data frame\n",
    "df = spark.sql(f\"\"\"\n",
    "   SELECT * FROM cde.wuv;\n",
    "   \"\"\"\n",
    "   )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of rows in the table before load\n",
    "if exists:\n",
    "    row_count_df = spark.sql(f\"SELECT count(*) as row_count FROM {databaseName}.{tableName}\")\n",
    "    row_count = row_count_df.first()[\"row_count\"]\n",
    "    \n",
    "    print(row_count)\n",
    "else:\n",
    "    print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf581a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount path location in data catalog\n",
    "mount_path = f'/mnt/pqr/{databaseName}/{tableName}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If table exists append the data to the table\n",
    "if exists:\n",
    "    df.write.format(\"delta\").mode(\"append\").save(mount_path)\n",
    "    print('Data appended')\n",
    "else:\n",
    "    #Overwrite the directory with the dataframe data\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(mount_path)\n",
    "     print('Data written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If table does not exist create table\n",
    "if not exists:\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE {databaseName}.{tableName}\n",
    "    USING DELTA LOCATION '{mount_path}'\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE {databaseName}.{tableName}\n",
    "    SET TBLPROPERTIES ('edl_sources' = 'klm')\n",
    "    \"\"\")\n",
    "    \n",
    "result = spark.sql(f\"\"\"\n",
    "     SELECT * FROM {databaseName}.{tableName}\n",
    "     \"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To delete data and drop table\n",
    "Drop table <Table_Name>\n",
    "\n",
    "dbutils.fs.rm(mount_path, recurse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
